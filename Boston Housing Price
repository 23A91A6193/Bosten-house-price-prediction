{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN85MOcAAzr4OOnZgtD+6yY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g53IQvme1E-1","executionInfo":{"status":"ok","timestamp":1754575665745,"user_tz":-330,"elapsed":9313,"user":{"displayName":"Raviteja V","userId":"06551818024185579526"}},"outputId":"9804bc1d-b770-4097-e250-5de8bd9ff6fc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n","\u001b[1m57026/57026\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"]}],"source":["import tensorflow as tf\n","from tensorflow.keras.datasets import boston_housing\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# --- Step 1: Get Our Data Ready (Boston Housing Prices) ---\n","\n","# Load the Boston Housing dataset. This dataset contains 13 different features\n","# (like crime rate, number of rooms, etc.) for houses and their median price.\n","# We're using 404 houses for training and 102 for testing.\n","(train_data, train_labels), (test_data, test_labels) = boston_housing.load_data()"]},{"cell_type":"code","source":["# The features in this dataset have very different scales (e.g., crime rate vs. number of rooms).\n","# A neural network performs better when all features are on a similar scale.\n","# We'll use a technique called \"normalization\" to standardize the data.\n","# Normalization: (value - mean) / standard deviation\n","mean = train_data.mean(axis=0)\n","train_data -= mean\n","std = train_data.std(axis=0)\n","train_data /= std\n","\n","# We must use the SAME mean and standard deviation from the training data to normalize the test data.\n","# This prevents \"data leakage\" from the test set into the model's training process.\n","test_data -= mean\n","test_data /= std\n","\n","print(f\"  Training data shape: {train_data.shape}\") # (404 houses, 13 features)\n","print(f\"  Training labels shape: {train_labels.shape}\") # (404 prices)\n","print(\"  Data normalized successfully.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JsHga5d33BBk","executionInfo":{"status":"ok","timestamp":1754576443153,"user_tz":-330,"elapsed":59,"user":{"displayName":"Raviteja V","userId":"06551818024185579526"}},"outputId":"48c0081b-928e-4e47-8722-b0e3c71ffcbc"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["  Training data shape: (404, 13)\n","  Training labels shape: (404,)\n","  Data normalized successfully.\n"]}]},{"cell_type":"code","source":["# Define some key sizes for our network layers\n","num_features = train_data.shape[1] # The number of input features (13 in this case)\n","\n","# We build our neural network layer by layer using Keras's Sequential model.\n","model = Sequential([\n","    # Layer 1: Hidden Dense Layer\n","    # This is a standard \"fully connected\" layer. It learns complex patterns from the input features.\n","    # '64' means it has 64 \"neurons\" or processing units.\n","    # 'activation='relu'' is a common \"activation function\" that helps the network learn\n","    # non-linear relationships.\n","    Dense(64, activation='relu', input_shape=(num_features,)),\n","\n","    # Layer 2: Another Hidden Dense Layer\n","    # Stacking a second layer allows the network to learn more complex patterns.\n","    Dense(64, activation='relu'),\n","\n","    # Layer 3: Output Dense Layer (The key difference for regression!)\n","    # This is our final prediction-making layer.\n","    # '1' means it has 1 neuron, because we want to predict a single continuous value (the price).\n","    # 'activation='linear'' (which is the default, so we can omit it) is used for regression.\n","    # It simply outputs the value it calculates, without squashing it between 0 and 1 like 'sigmoid' or 'softmax'.\n","    Dense(1, activation='linear')\n","])\n","\n","# Print a summary of the model's layers, output shapes, and number of parameters.\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":286},"id":"D16AlCee6IDO","executionInfo":{"status":"ok","timestamp":1754576760811,"user_tz":-330,"elapsed":167,"user":{"displayName":"Raviteja V","userId":"06551818024185579526"}},"outputId":"e6a30022-c882-453f-a7f2-355b8ce97133"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"sequential\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m896\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,121\u001b[0m (20.00 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,121</span> (20.00 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,121\u001b[0m (20.00 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,121</span> (20.00 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"code","source":["# --- Step 3: Prepare the Network for Training (Compilation) ---\n","\n","# Before training, we need to configure our model:\n","# 'optimizer='adam'': This is the algorithm that adjusts the network's internal settings (weights)\n","#                     during training to minimize the 'loss'.\n","# 'loss='mse'': This is the \"cost function\" or \"error function\". For regression,\n","#               Mean Squared Error (MSE) is a standard choice. It measures the average\n","#               squared difference between our predictions and the actual prices.\n","# 'metrics=['mae']': This tells Keras to calculate and report the 'Mean Absolute Error' (MAE),\n","#                    which is easier to interpret than MSE. MAE tells us the average\n","#                    absolute difference between our predictions and the actual prices.\n","model.compile(optimizer='adam',\n","              loss='mse', # Mean Squared Error\n","              metrics=['mae']) # Mean Absolute Error\n","print(\"Model compiled successfully! Ready to train.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l3ZUSzdq8P6W","executionInfo":{"status":"ok","timestamp":1754577193114,"user_tz":-330,"elapsed":72,"user":{"displayName":"Raviteja V","userId":"06551818024185579526"}},"outputId":"f970456f-e6f8-4f5e-e16b-f427f2edb7de"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Model compiled successfully! Ready to train.\n"]}]},{"cell_type":"code","source":["# --- Step 4: Train Our Network! ---\n","\n","# This is where the model learns from the training data.\n","# We'll save the training history to create our graphs later.\n","# 'epochs=100': We will go through the training data 100 times. Regression often\n","#               requires more epochs than classification to converge.\n","# 'batch_size=32': The model will process 32 houses at a time.\n","# 'validation_split=0.2': We use 20% of the training data as a validation set to\n","#                         monitor for overfitting.\n","history = model.fit(train_data,\n","                    train_labels,\n","                    epochs=100,\n","                    batch_size=32,\n","                    validation_split=0.2,\n","                    verbose=1)\n","\n","print(\"Model training complete!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l6az6Yfr8nPp","executionInfo":{"status":"ok","timestamp":1754577220626,"user_tz":-330,"elapsed":16518,"user":{"displayName":"Raviteja V","userId":"06551818024185579526"}},"outputId":"6556cccd-6bc4-4126-a567-8b219a449930"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 563.4676 - mae: 21.6938 - val_loss: 586.3892 - val_mae: 22.3850\n","Epoch 2/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 504.5686 - mae: 20.4809 - val_loss: 538.7241 - val_mae: 21.2828\n","Epoch 3/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 450.6087 - mae: 19.1517 - val_loss: 477.1283 - val_mae: 19.7991\n","Epoch 4/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 413.8264 - mae: 17.8866 - val_loss: 396.8596 - val_mae: 17.7646\n","Epoch 5/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 344.8120 - mae: 15.9369 - val_loss: 302.2987 - val_mae: 15.0055\n","Epoch 6/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 243.6955 - mae: 13.1514 - val_loss: 207.3840 - val_mae: 11.7798\n","Epoch 7/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 154.7846 - mae: 9.7469 - val_loss: 132.2004 - val_mae: 8.6986\n","Epoch 8/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 91.3012 - mae: 7.0837 - val_loss: 88.6710 - val_mae: 7.0977\n","Epoch 9/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 69.8617 - mae: 6.1450 - val_loss: 69.3196 - val_mae: 6.3252\n","Epoch 10/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 48.2827 - mae: 5.1461 - val_loss: 55.2273 - val_mae: 5.5665\n","Epoch 11/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 36.5511 - mae: 4.4059 - val_loss: 44.8655 - val_mae: 4.9144\n","Epoch 12/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 33.3309 - mae: 4.1005 - val_loss: 36.9269 - val_mae: 4.3422\n","Epoch 13/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 26.2486 - mae: 3.6339 - val_loss: 30.6214 - val_mae: 3.9874\n","Epoch 14/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 33.4870 - mae: 3.7751 - val_loss: 26.6370 - val_mae: 3.8169\n","Epoch 15/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 24.0413 - mae: 3.3043 - val_loss: 24.2244 - val_mae: 3.6551\n","Epoch 16/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 24.0590 - mae: 3.3808 - val_loss: 22.4124 - val_mae: 3.5393\n","Epoch 17/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 24.0173 - mae: 3.1885 - val_loss: 20.9568 - val_mae: 3.4521\n","Epoch 18/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 23.0460 - mae: 3.2754 - val_loss: 19.8432 - val_mae: 3.3772\n","Epoch 19/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 21.4691 - mae: 3.2516 - val_loss: 19.0967 - val_mae: 3.3043\n","Epoch 20/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 23.1549 - mae: 3.1365 - val_loss: 18.3466 - val_mae: 3.2671\n","Epoch 21/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 16.6688 - mae: 2.9716 - val_loss: 17.6441 - val_mae: 3.1981\n","Epoch 22/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 16.7536 - mae: 2.9690 - val_loss: 17.0559 - val_mae: 3.1548\n","Epoch 23/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 17.4306 - mae: 2.9292 - val_loss: 16.6343 - val_mae: 3.1052\n","Epoch 24/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 15.3330 - mae: 2.7441 - val_loss: 16.0756 - val_mae: 3.0314\n","Epoch 25/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 17.3775 - mae: 2.8402 - val_loss: 15.7032 - val_mae: 2.9930\n","Epoch 26/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 14.6742 - mae: 2.7088 - val_loss: 15.5848 - val_mae: 3.0222\n","Epoch 27/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 15.8881 - mae: 2.8154 - val_loss: 15.2191 - val_mae: 2.9862\n","Epoch 28/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 14.3060 - mae: 2.7138 - val_loss: 16.0055 - val_mae: 3.0288\n","Epoch 29/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 16.0781 - mae: 2.9126 - val_loss: 15.2861 - val_mae: 2.9321\n","Epoch 30/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 12.7571 - mae: 2.6289 - val_loss: 14.7087 - val_mae: 2.8956\n","Epoch 31/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 13.6320 - mae: 2.6003 - val_loss: 14.5878 - val_mae: 2.8858\n","Epoch 32/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 13.8071 - mae: 2.6632 - val_loss: 14.2741 - val_mae: 2.8530\n","Epoch 33/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 14.5660 - mae: 2.6854 - val_loss: 14.1402 - val_mae: 2.8274\n","Epoch 34/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 12.2037 - mae: 2.5126 - val_loss: 13.9448 - val_mae: 2.7947\n","Epoch 35/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 14.1054 - mae: 2.7190 - val_loss: 13.7631 - val_mae: 2.7757\n","Epoch 36/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 10.9268 - mae: 2.4457 - val_loss: 13.7939 - val_mae: 2.7733\n","Epoch 37/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 12.9832 - mae: 2.6695 - val_loss: 14.0268 - val_mae: 2.7716\n","Epoch 38/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 11.8925 - mae: 2.4256 - val_loss: 13.7276 - val_mae: 2.7337\n","Epoch 39/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 11.6272 - mae: 2.5227 - val_loss: 14.4110 - val_mae: 2.7982\n","Epoch 40/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 13.5341 - mae: 2.6824 - val_loss: 13.7641 - val_mae: 2.7699\n","Epoch 41/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 11.5386 - mae: 2.5045 - val_loss: 13.2297 - val_mae: 2.6987\n","Epoch 42/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 12.2795 - mae: 2.5039 - val_loss: 12.7970 - val_mae: 2.6924\n","Epoch 43/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 13.2915 - mae: 2.4918 - val_loss: 12.5750 - val_mae: 2.6556\n","Epoch 44/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 10.4844 - mae: 2.2580 - val_loss: 12.4431 - val_mae: 2.7084\n","Epoch 45/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 13.2114 - mae: 2.4514 - val_loss: 12.3585 - val_mae: 2.6602\n","Epoch 46/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 10.8593 - mae: 2.3788 - val_loss: 12.5038 - val_mae: 2.6315\n","Epoch 47/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 11.7634 - mae: 2.4640 - val_loss: 12.5478 - val_mae: 2.6412\n","Epoch 48/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 11.1023 - mae: 2.3837 - val_loss: 12.5955 - val_mae: 2.7231\n","Epoch 49/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 13.3058 - mae: 2.4848 - val_loss: 12.5151 - val_mae: 2.7288\n","Epoch 50/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 11.3174 - mae: 2.3657 - val_loss: 12.3240 - val_mae: 2.6535\n","Epoch 51/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 10.7198 - mae: 2.3562 - val_loss: 13.0250 - val_mae: 2.6549\n","Epoch 52/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 14.9374 - mae: 2.6221 - val_loss: 12.3951 - val_mae: 2.6487\n","Epoch 53/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 10.1143 - mae: 2.2450 - val_loss: 12.1269 - val_mae: 2.6491\n","Epoch 54/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 10.4026 - mae: 2.1403 - val_loss: 12.1581 - val_mae: 2.6498\n","Epoch 55/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 8.7683 - mae: 2.1814 - val_loss: 12.1179 - val_mae: 2.6145\n","Epoch 56/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 9.8630 - mae: 2.2491 - val_loss: 12.6716 - val_mae: 2.7174\n","Epoch 57/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 9.8063 - mae: 2.1918 - val_loss: 12.3197 - val_mae: 2.6388\n","Epoch 58/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.3819 - mae: 2.0883 - val_loss: 12.3040 - val_mae: 2.6216\n","Epoch 59/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 9.0189 - mae: 2.1971 - val_loss: 12.6477 - val_mae: 2.6748\n","Epoch 60/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.4603 - mae: 2.2009 - val_loss: 12.7933 - val_mae: 2.7520\n","Epoch 61/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.8206 - mae: 2.1966 - val_loss: 12.6637 - val_mae: 2.7013\n","Epoch 62/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7.7259 - mae: 2.0784 - val_loss: 14.6027 - val_mae: 2.6810\n","Epoch 63/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.8188 - mae: 2.2654 - val_loss: 14.2400 - val_mae: 2.6854\n","Epoch 64/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 11.0125 - mae: 2.3335 - val_loss: 13.8290 - val_mae: 2.6587\n","Epoch 65/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 10.8053 - mae: 2.2287 - val_loss: 13.4509 - val_mae: 2.7069\n","Epoch 66/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 10.7023 - mae: 2.3291 - val_loss: 13.2630 - val_mae: 2.6605\n","Epoch 67/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 7.6514 - mae: 2.0345 - val_loss: 13.2100 - val_mae: 2.6903\n","Epoch 68/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 9.0883 - mae: 2.1923 - val_loss: 13.3515 - val_mae: 2.6603\n","Epoch 69/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.5317 - mae: 2.1614 - val_loss: 13.0298 - val_mae: 2.6539\n","Epoch 70/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 7.9298 - mae: 2.0458 - val_loss: 13.0145 - val_mae: 2.6674\n","Epoch 71/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 7.7820 - mae: 2.0559 - val_loss: 13.1195 - val_mae: 2.6579\n","Epoch 72/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 10.7694 - mae: 2.1237 - val_loss: 13.3778 - val_mae: 2.6776\n","Epoch 73/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7.3190 - mae: 1.9599 - val_loss: 14.0713 - val_mae: 2.7413\n","Epoch 74/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.7134 - mae: 2.1039 - val_loss: 14.1624 - val_mae: 2.6708\n","Epoch 75/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 9.2167 - mae: 2.2294 - val_loss: 13.8936 - val_mae: 2.6662\n","Epoch 76/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7.5064 - mae: 2.0478 - val_loss: 13.5386 - val_mae: 2.6750\n","Epoch 77/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 10.1254 - mae: 2.1984 - val_loss: 13.8154 - val_mae: 2.6913\n","Epoch 78/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 8.0523 - mae: 2.0612 - val_loss: 13.1595 - val_mae: 2.6542\n","Epoch 79/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 11.5283 - mae: 2.2338 - val_loss: 13.5672 - val_mae: 2.6414\n","Epoch 80/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 9.0012 - mae: 2.1129 - val_loss: 13.4553 - val_mae: 2.6455\n","Epoch 81/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.4286 - mae: 2.0441 - val_loss: 13.2574 - val_mae: 2.5905\n","Epoch 82/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 9.5588 - mae: 2.2624 - val_loss: 13.5846 - val_mae: 2.6938\n","Epoch 83/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 10.8324 - mae: 2.2076 - val_loss: 13.6322 - val_mae: 2.6545\n","Epoch 84/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 10.7450 - mae: 2.1893 - val_loss: 13.5884 - val_mae: 2.6515\n","Epoch 85/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7.6803 - mae: 1.9886 - val_loss: 13.3226 - val_mae: 2.6365\n","Epoch 86/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7.8962 - mae: 2.0314 - val_loss: 13.4268 - val_mae: 2.6266\n","Epoch 87/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 6.9159 - mae: 1.9423 - val_loss: 13.6720 - val_mae: 2.6308\n","Epoch 88/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.7713 - mae: 2.0347 - val_loss: 14.2544 - val_mae: 2.6659\n","Epoch 89/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7.7697 - mae: 2.0995 - val_loss: 13.8299 - val_mae: 2.6565\n","Epoch 90/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7.2130 - mae: 1.9639 - val_loss: 13.6032 - val_mae: 2.6018\n","Epoch 91/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 7.5870 - mae: 2.0369 - val_loss: 13.9335 - val_mae: 2.6642\n","Epoch 92/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7.3079 - mae: 1.9582 - val_loss: 13.8493 - val_mae: 2.6566\n","Epoch 93/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.8205 - mae: 2.0922 - val_loss: 14.5358 - val_mae: 2.6549\n","Epoch 94/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 6.5382 - mae: 1.8850 - val_loss: 13.8969 - val_mae: 2.5916\n","Epoch 95/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7.5958 - mae: 2.0733 - val_loss: 14.2017 - val_mae: 2.6292\n","Epoch 96/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 9.0899 - mae: 2.1448 - val_loss: 14.8044 - val_mae: 2.7351\n","Epoch 97/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7.0529 - mae: 1.9448 - val_loss: 13.6751 - val_mae: 2.6090\n","Epoch 98/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7.9393 - mae: 2.0136 - val_loss: 14.0325 - val_mae: 2.6250\n","Epoch 99/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 7.6985 - mae: 2.0141 - val_loss: 14.5221 - val_mae: 2.6920\n","Epoch 100/100\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7.4272 - mae: 1.9561 - val_loss: 14.9442 - val_mae: 2.6685\n","Model training complete!\n"]}]},{"cell_type":"code","source":["# --- Step 5: Evaluate Our Network (How Good Is It?) ---\n","\n","# We evaluate the model on the 'test_data', which was completely unseen during training.\n","# This gives us an unbiased measure of its performance.\n","loss, mae = model.evaluate(test_data, test_labels, verbose=2)\n","\n","print(f\"\\nFinal Test MSE (Loss): {loss:.2f}\")\n","print(f\"Final Test MAE (Mean Absolute Error): {mae:.2f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i1p3dBqd9w_g","executionInfo":{"status":"ok","timestamp":1754577455310,"user_tz":-330,"elapsed":148,"user":{"displayName":"Raviteja V","userId":"06551818024185579526"}},"outputId":"77b9d147-365d-4e82-af4a-cfb918a4cacf"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["4/4 - 0s - 15ms/step - loss: 26.4707 - mae: 3.2047\n","\n","Final Test MSE (Loss): 26.47\n","Final Test MAE (Mean Absolute Error): 3.20\n"]}]},{"cell_type":"code","source":["# We can also calculate R-squared, a common metric for regression.\n","# R-squared measures the proportion of the variance in the dependent variable\n","# (house price) that is predictable from the independent variables (features).\n","# A value of 1.0 means the model perfectly predicts the variance.\n","y_pred = model.predict(test_data).flatten()\n","y_true = test_labels\n","\n","ss_total = np.sum((y_true - np.mean(y_true)) ** 2)\n","ss_residual = np.sum((y_true - y_pred) ** 2)\n","r_squared = 1 - (ss_residual / ss_total)\n","\n","print(f\"Final Test R-squared ($R^2$): {r_squared:.4f}\")\n","\n","y_pred1 = model.predict(train_data).flatten()\n","y_training = train_labels\n","\n","ss_total = np.sum((y_training - np.mean(y_training)) ** 2)\n","ss_residual = np.sum((y_training - y_pred1) ** 2)\n","r_squared = 1 - (ss_residual / ss_total)\n","\n","print(f\"Final Test R-squared ($R^2$): {r_squared:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0GRW3mZdEusa","executionInfo":{"status":"ok","timestamp":1754579614781,"user_tz":-330,"elapsed":355,"user":{"displayName":"Raviteja V","userId":"06551818024185579526"}},"outputId":"74a19b1c-706b-4d02-dc22-98f727686aa0"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n","Final Test R-squared ($R^2$): 0.6820\n","\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n","Final Test R-squared ($R^2$): 0.8916\n"]}]},{"cell_type":"code","source":["# --- Example: Making a Prediction on a single house ---\n","print(\"\\n--- Example: Making a Prediction on a Sample House ---\")\n","# Let's take the first house from our test dataset and see what the model predicts.\n","# We need to reshape it to be a \"batch\" of 1 house for the model.\n","sample_house_for_prediction = test_data[0:1]\n","true_price = test_labels[0]\n","\n","# Get the model's prediction\n","predicted_price = model.predict(sample_house_for_prediction)[0][0]\n","\n","print(f\"  Actual price for the first test house: ${true_price * 1000:,.2f}\")\n","print(f\"  Model's predicted price: ${predicted_price * 1000:,.2f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"73ubwTNqGO1U","executionInfo":{"status":"ok","timestamp":1754579725415,"user_tz":-330,"elapsed":149,"user":{"displayName":"Raviteja V","userId":"06551818024185579526"}},"outputId":"5195d04b-2815-4cef-f95f-ae7d1542c0b8"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Example: Making a Prediction on a Sample House ---\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n","  Actual price for the first test house: $7,200.00\n","  Model's predicted price: $8,033.12\n"]}]}]}